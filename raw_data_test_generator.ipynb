{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T14:45:40.398203Z",
     "iopub.status.busy": "2025-07-28T14:45:40.397918Z",
     "iopub.status.idle": "2025-07-28T14:45:40.689564Z",
     "shell.execute_reply": "2025-07-28T14:45:40.688985Z",
     "shell.execute_reply.started": "2025-07-28T14:45:40.398166Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Excel file...\n",
      "Workbook shape: (1830, 10)\n",
      "Date range     : 2000-06-01  →  2014-09-30\n",
      "\n",
      "Split sizes:\n",
      "  train : (732, 10)\n",
      "  test  : (488, 10)\n",
      "Saved 449 samples for Inflow  →  inflow\n",
      "Saved 449 samples for Sundargarh  →  sundargarh\n",
      "Saved 449 samples for Kurubhata  →  kurubhata\n",
      "Saved 449 samples for Basantpur  →  basantpur\n",
      "Saved 449 samples for Ghatora  →  ghatora\n",
      "Saved 449 samples for Kelo  →  kelo\n",
      "Saved 449 samples for Paramanpur  →  paramanpur\n",
      "Saved 449 samples for Simga  →  simga\n",
      "Saved 449 samples for Rajim  →  rajim\n",
      "\n",
      "Done!  All raw test‑set files are in: Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_w/4xfs9wld05nfzlbq4p03y6z80000gn/T/ipykernel_51509/1945376082.py:47: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[DATE_COLUMN] = pd.to_datetime(df[DATE_COLUMN])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "#  Raw‑test‑set generator  |  Hirakud & upstream stations\n",
    "#  ------------------------------------------------------------\n",
    "#  • No smoothing, no scaling – pure values from the sheet\n",
    "#  • Produces y_test arrays shaped (samples, 5) for every gauge\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy  import array\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "\n",
    "SEED            = 42\n",
    "EXCEL_FILE      = \"Data/new data.xlsx\"   # <-- adjust if the folder/name differ\n",
    "DATE_COLUMN     = \"Date\"                                  # column containing the calendar date\n",
    "RAINFALL_COL    = \"Mean_areal_rainfall_upper\"             # rainfall column name\n",
    "\n",
    "# every discharge / water‑level column you want a raw test‑set for\n",
    "STATIONS = [\n",
    "    \"Inflow\",        # Hirakud reservoir inflow  (note the space in your sheet)\n",
    "    \"Sundargarh\",\n",
    "    \"Kurubhata\",\n",
    "    \"Basantpur\",\n",
    "    \"Ghatora\",\n",
    "    \"Kelo\",\n",
    "    \"Paramanpur\",\n",
    "    \"Simga\",\n",
    "    \"Rajim\"\n",
    "]\n",
    "\n",
    "# sequence lengths\n",
    "N_STEPS_IN  = 30    # look‑back window (days)\n",
    "N_STEPS_OUT = 10     # forecast horizon (days)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# 1. ------------ READ THE WORKBOOK (no smoothing / scaling) --\n",
    "print(\"Reading Excel file...\")\n",
    "use_cols = [DATE_COLUMN, RAINFALL_COL] + STATIONS\n",
    "df = pd.read_excel(EXCEL_FILE, usecols=use_cols, engine=\"openpyxl\")\n",
    "\n",
    "# basic clean‑up\n",
    "df[DATE_COLUMN] = pd.to_datetime(df[DATE_COLUMN])\n",
    "df.set_index(DATE_COLUMN, inplace=True)\n",
    "df.ffill(inplace=True); df.bfill(inplace=True)\n",
    "\n",
    "print(f\"Workbook shape: {df.shape}\")\n",
    "print(f\"Date range     : {df.index.min().date()}  →  {df.index.max().date()}\")\n",
    "\n",
    "# 2. ------------ TRAIN / TEST DATE SPLIT --------------------\n",
    "train_df = df.loc[\"2005-01-01\" : \"2010-12-31\"]\n",
    "test_df  = df.loc[\"2011-01-01\" : \"2014-12-31\"]\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "print(\"  train :\", train_df.shape)\n",
    "print(\"  test  :\", test_df.shape)\n",
    "\n",
    "# 3. ------------ SEQUENCE SPLITTER --------------------------\n",
    "def split_sequences(data, steps_in, steps_out, target_idx):\n",
    "    \"\"\"Return X, y arrays where y is the target column only.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)):\n",
    "        end_ix = i + steps_in\n",
    "        out_end = end_ix + steps_out\n",
    "        if out_end > len(data):\n",
    "            break\n",
    "        seq_x = data[i:end_ix, :]\n",
    "        seq_y = data[end_ix:out_end, target_idx]\n",
    "        X.append(seq_x);  y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "# 4. ------------ LOOP OVER EVERY STATION --------------------\n",
    "OUT_DIR = Path(\"Data\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for tgt in STATIONS:\n",
    "    target_idx   = test_df.columns.get_loc(tgt)\n",
    "    X_test, y_test = split_sequences(test_df.values,\n",
    "                                     N_STEPS_IN,\n",
    "                                     N_STEPS_OUT,\n",
    "                                     target_idx)\n",
    "\n",
    "    # filenames friendly to Linux\n",
    "    safe_name = tgt.strip().lower().replace(\" \", \"_\")\n",
    "\n",
    "    # save .npy\n",
    "    np.save(OUT_DIR / f\"raw_test_y_{safe_name}.npy\", y_test)\n",
    "\n",
    "    # save .csv for easy inspection\n",
    "    cols = [f\"{safe_name}_t+{i+1}\" for i in range(N_STEPS_OUT)]\n",
    "    pd.DataFrame(y_test, columns=cols).to_csv(\n",
    "        OUT_DIR / f\"raw_test_y_{safe_name}_10_steps_ahead.csv\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    print(f\"Saved {y_test.shape[0]} samples for {tgt.strip()}  →  {safe_name}\")\n",
    "\n",
    "print(\"\\nDone!  All raw test‑set files are in:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7845267,
     "sourceId": 12437325,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7939697,
     "sourceId": 12572394,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959853,
     "sourceId": 12601992,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959857,
     "sourceId": 12601998,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959868,
     "sourceId": 12602014,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959881,
     "sourceId": 12602035,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8003950,
     "sourceId": 12665830,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
