{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available regions: 'Hirakud', 'Basantpur', 'Ghatora', 'Kurubhata', 'Rajim', 'Simga', 'Sundargarh'\n",
    "REGION = 'Hirakud'\n",
    "\n",
    "# Available percentage for training: 20, 50, 80, 100\n",
    "training_percentage = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Base paths\n",
    "DATA_DIR = Path('Data')\n",
    "MODEL_DIR = Path('models')\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model and training parameters\n",
    "N_STEPS_IN, N_STEPS_OUT = 30, 10 # Look back 30 days to predict next 5 days\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_w/4xfs9wld05nfzlbq4p03y6z80000gn/T/ipykernel_44007/586350803.py:70: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data['Date'] = pd.to_datetime(data['Date'])\n"
     ]
    }
   ],
   "source": [
    "# Available regions: 'Hirakud', 'Basantpur', 'Ghatora', 'Kurubhata', 'Rajim', 'Simga', 'Sundargarh'\n",
    "# REGION = 'Kurubhata'\n",
    "\n",
    "# Base configuration\n",
    "BASE_FEATURES = {\n",
    "    # 'Hirakud': {\n",
    "    #     'features': ['Mean_areal_rainfall_upper', 'Inflow', 'Sundargarh', 'Kurubhata'],\n",
    "    #     'target': 'Inflow'\n",
    "    # },\n",
    "    'Hirakud': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Inflow'],\n",
    "        'target': 'Inflow'\n",
    "    },\n",
    "    'Basantpur': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Basantpur'],\n",
    "        'target': 'Basantpur'\n",
    "    },\n",
    "    # 'Basantpur': {\n",
    "    #     'features': ['Mean_areal_rainfall_upper', 'Basantpur', 'Rajim', 'Simga', 'Ghatora'],\n",
    "    #     'target': 'Basantpur'\n",
    "    # },\n",
    "    'Ghatora': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Ghatora'],\n",
    "        'target': 'Ghatora'\n",
    "    },\n",
    "    'Kurubhata': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Kurubhata'],\n",
    "        'target': 'Kurubhata'\n",
    "    },\n",
    "    'Rajim': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Rajim'],\n",
    "        'target': 'Rajim'\n",
    "    },\n",
    "    'Simga': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Simga'],\n",
    "        'target': 'Simga'\n",
    "    },\n",
    "    'Sundargarh': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Sundargarh'],\n",
    "        'target': 'Sundargarh'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate output columns dynamically\n",
    "def get_output_columns(features, region):\n",
    "    columns = {'Mean_areal_rainfall_upper': 'rainfall'}\n",
    "    for feat in features:\n",
    "        if feat != 'Mean_areal_rainfall_upper':\n",
    "            columns[feat] = feat.lower() + ('_discharge' if feat != 'rainfall' else '')\n",
    "    return columns\n",
    "\n",
    "# Build final config\n",
    "current_config = BASE_FEATURES.get(REGION)\n",
    "if not current_config:\n",
    "    raise ValueError(f\"Configuration for region '{REGION}' not found.\")\n",
    "\n",
    "# Set global variables\n",
    "FEATURES = current_config['features']\n",
    "TARGET_FEATURE = current_config['target']\n",
    "OUTPUT_COLUMNS = get_output_columns(FEATURES, REGION)\n",
    "TARGET_FEATURE_NAME = OUTPUT_COLUMNS[TARGET_FEATURE]\n",
    "FEATURES_TO_USE = list(OUTPUT_COLUMNS.values())\n",
    "MODEL_SAVE_PATH = f\"best_mlp_mann_model_{REGION}.pth\"\n",
    "RAW_TEST_FILE_PATH = f\"Data/raw_test_y_{REGION.lower()}.csv\"\n",
    "\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_excel('Data/new data.xlsx')\n",
    "data = data[['Date'] + FEATURES].rename(columns=OUTPUT_COLUMNS)\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.set_index('Date').ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rainfall</th>\n",
       "      <th>inflow_discharge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-06-01</th>\n",
       "      <td>1.896728</td>\n",
       "      <td>39.6763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-06-02</th>\n",
       "      <td>1.223008</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-06-03</th>\n",
       "      <td>2.578513</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-06-04</th>\n",
       "      <td>1.107180</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-06-05</th>\n",
       "      <td>1.633354</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rainfall  inflow_discharge\n",
       "Date                                  \n",
       "2000-06-01  1.896728           39.6763\n",
       "2000-06-02  1.223008            0.0000\n",
       "2000-06-03  2.578513            0.0000\n",
       "2000-06-04  1.107180            0.0000\n",
       "2000-06-05  1.633354            0.0000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(data, training_percentage=100):\n",
    "    # Define the split points based on percentage\n",
    "    split_dates = {\n",
    "        100: '2000-01-01',  # 100% of data -> 4017 days\n",
    "        80: '2002-01-01',   # 80% of data -> 3286 days\n",
    "        50: '2005-01-01',    # 50% of data -> 2190 days\n",
    "        20: '2008-01-01'\n",
    "    }\n",
    "    \n",
    "    if training_percentage not in split_dates:\n",
    "        raise ValueError(\"training_percentage must be one of: 50, 80, or 100\")      # Would like to change to a continous form\n",
    "    \n",
    "    # Split the data\n",
    "    train_start = split_dates[training_percentage]\n",
    "    data_train = data.loc[train_start:'2010-12-31'].copy()\n",
    "    data_test = data.loc['2011-01-01':'2014-12-31'].copy()\n",
    "    \n",
    "    return data_train, data_test\n",
    "\n",
    "# usage:\n",
    "# training_percentage = 50\n",
    "data_train, data_test = get_train_test_split(data, training_percentage)\n",
    "\n",
    "# Keep raw test target before processing\n",
    "raw_data_test_target = data_test[[TARGET_FEATURE_NAME]].copy()\n",
    "\n",
    "# Smooth features\n",
    "for col in FEATURES_TO_USE:\n",
    "    window = np.blackman(20)\n",
    "    data_train[col] = np.convolve(window/window.sum(), data_train[col].values, 'same')\n",
    "    data_test[col] = np.convolve(window/window.sum(), data_test[col].values, 'same')\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(data_train)\n",
    "train_scaled = scaler.transform(data_train)\n",
    "test_scaled = scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the target column in the feature set\n",
    "target_col_index = FEATURES_TO_USE.index(TARGET_FEATURE_NAME)\n",
    "\n",
    "def create_sequences(data, n_steps_in, n_steps_out, target_idx):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps_in - n_steps_out + 1):\n",
    "        X.append(data[i:i+n_steps_in])\n",
    "        y.append(data[i+n_steps_in:i+n_steps_in+n_steps_out, target_idx])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(train_scaled, N_STEPS_IN, N_STEPS_OUT, target_col_index)\n",
    "\n",
    "# Split into train/validation\n",
    "train_X, val_X, train_y, val_y = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_data = TensorDataset(\n",
    "    torch.tensor(train_X, dtype=torch.float32),\n",
    "    torch.tensor(train_y, dtype=torch.float32)\n",
    ")\n",
    "val_data = TensorDataset(\n",
    "    torch.tensor(val_X, dtype=torch.float32),\n",
    "    torch.tensor(val_y, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1303, 30, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1303, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. MODEL DEFINITION (MLP+MANN) ---\n",
    "class MLP_MANN_Model(nn.Module):\n",
    "    def __init__(self, input_dim, n_steps_in, n_steps_out, memory_size=128, memory_dim=64, hidden_dim=256):\n",
    "        super(MLP_MANN_Model, self).__init__()\n",
    "        flattened_input_dim = n_steps_in * input_dim\n",
    "        self.memory = nn.Parameter(torch.randn(memory_size, memory_dim))\n",
    "        nn.init.xavier_uniform_(self.memory)\n",
    "        self.input_projection = nn.Linear(flattened_input_dim, memory_dim)\n",
    "        self.dense1 = nn.Linear(flattened_input_dim + memory_dim, hidden_dim)\n",
    "        self.dense2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.output_layer = nn.Linear(hidden_dim // 2, n_steps_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        flat_x = x.view(x.size(0), -1)\n",
    "        projected_input = self.input_projection(flat_x)\n",
    "        attention_scores = torch.matmul(projected_input, self.memory.t())\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        memory_output = torch.matmul(attention_weights, self.memory)\n",
    "        augmented_input = torch.cat([flat_x, memory_output], dim=1)\n",
    "        h1 = self.relu(self.dense1(augmented_input))\n",
    "        h2 = self.relu(self.dense2(h1))\n",
    "        output = self.output_layer(h2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, batch_X, batch_y, criterion, optimizer, device):\n",
    "    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(batch_X)\n",
    "    loss = criterion(outputs, batch_y)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for X_batch, y_batch in data_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        total_loss += criterion(outputs, y_batch).item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, criterion, optimizer, device, model_path, save_model=True):\n",
    "    model = model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train': [], 'val': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            train_loss += train_step(model, batch_X, batch_y, criterion, optimizer, device)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Save best model if save_model is True\n",
    "        if save_model and val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "        # Record history\n",
    "        history['train'].append(train_loss)\n",
    "        history['val'].append(val_loss)\n",
    "        \n",
    "        # print(f\"Epoch {epoch+1:3d}/{epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# Training setup\n",
    "input_dim = X.shape[-1]\n",
    "model = MLP_MANN_Model(input_dim, N_STEPS_IN, N_STEPS_OUT)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create model path with region name\n",
    "model_path = MODEL_DIR / f\"best_mlp_mann_model_{REGION}_{training_percentage}pct.pth\"\n",
    "\n",
    "# Train the model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    model_path=str(model_path),\n",
    "    save_model=True  # Set to False to disable model saving\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 6. POST-TRAINING ANALYSIS ---\n",
    "# print(\"\\n--- 6. Plotting Training and Validation Loss ---\")\n",
    "# if history:  # Check if history exists\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.plot(history['train'], label='Train Loss', color='blue')\n",
    "#     plt.plot(history['val'], label='Validation Loss', color='orange')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Loss (MSE)')\n",
    "#     plt.title('MLP+MANN (Hirakud): Training and Validation Loss', fontsize=16)\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (450,5) (449,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m y_predict_smoothed \u001b[38;5;241m=\u001b[39m test_predictions_inv[:num_samples]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Apply offset\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m offset \u001b[38;5;241m=\u001b[39m y_test_raw \u001b[38;5;241m-\u001b[39m y_test_smoothed\n\u001b[1;32m     39\u001b[0m y_predict_adjusted \u001b[38;5;241m=\u001b[39m y_predict_smoothed \u001b[38;5;241m+\u001b[39m offset\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (450,5) (449,10) "
     ]
    }
   ],
   "source": [
    "# --- 7. FINAL EVALUATION ---\n",
    "# Create test sequences\n",
    "X_test, y_test_scaled = create_sequences(test_scaled, N_STEPS_IN, N_STEPS_OUT, target_col_index)\n",
    "# print(f\"Testing on {X_test.shape[0]} samples\")\n",
    "\n",
    "# Load best model from the models directory\n",
    "model_path = MODEL_DIR / f\"best_mlp_mann_model_{REGION}_{training_percentage}pct.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device).eval()\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    test_predictions_scaled = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Inverse scaling function\n",
    "def inverse_scale_data(scaled_data, original_scaler, n_features, target_idx):\n",
    "    dummy = np.zeros((scaled_data.size, n_features))\n",
    "    dummy[:, target_idx] = scaled_data.flatten()\n",
    "    return original_scaler.inverse_transform(dummy)[:, target_idx].reshape(scaled_data.shape)\n",
    "\n",
    "# Apply inverse scaling\n",
    "n_features = train_scaled.shape[1]\n",
    "y_test_inv = inverse_scale_data(y_test_scaled, scaler, n_features, target_col_index)\n",
    "test_predictions_inv = inverse_scale_data(test_predictions_scaled, scaler, n_features, target_col_index)\n",
    "\n",
    "# Load raw test data\n",
    "raw_y_test = pd.read_csv(RAW_TEST_FILE_PATH, header=0, dtype=float).values\n",
    "num_samples = raw_y_test.shape[0]\n",
    "# print(f\"Loaded raw test data with {num_samples} samples\")\n",
    "\n",
    "# Align and calculate offset\n",
    "y_test_raw = raw_y_test[:num_samples]\n",
    "y_test_smoothed = y_test_inv[:num_samples]\n",
    "y_predict_smoothed = test_predictions_inv[:num_samples]\n",
    "\n",
    "# Apply offset\n",
    "offset = y_test_raw - y_test_smoothed\n",
    "y_predict_adjusted = y_predict_smoothed + offset\n",
    "# print(\"Offset calculation and adjustment complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict_smoothed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(observed, predicted):\n",
    "    \"\"\"Calculate NSE, RSR, PBIAS, EVOL, PE, and TPE metrics.\n",
    "    \n",
    "    Args:\n",
    "        observed: Array of observed values\n",
    "        predicted: Array of predicted values\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (NSE, RSR, PBIAS, EVOL, PE, TPE)\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if len(observed) == 0 or len(predicted) == 0 or len(observed) != len(predicted):\n",
    "        return -np.inf, np.inf, np.inf, np.inf, np.inf, np.inf\n",
    "    \n",
    "    # 1. Calculate NSE (Nash-Sutcliffe Efficiency)\n",
    "    observed_mean = np.mean(observed)\n",
    "    numerator = np.sum((observed - predicted) ** 2)\n",
    "    denominator = np.sum((observed - observed_mean) ** 2)\n",
    "    nse = 1 - (numerator / denominator) if denominator != 0 else -np.inf\n",
    "    \n",
    "    # 2. Calculate RSR (Root Mean Square Error - Observations Standard deviation Ratio)\n",
    "    rsr_numerator = np.sqrt(np.sum((observed - predicted) ** 2))\n",
    "    rsr_denominator = np.sqrt(np.sum((observed - observed_mean) ** 2))\n",
    "    rsr = rsr_numerator / rsr_denominator if rsr_denominator != 0 else np.inf\n",
    "    \n",
    "    # 3. Calculate PBIAS (Percent Bias)\n",
    "    pbias = (np.sum(predicted - observed) / np.sum(observed)) * 100 if np.sum(observed) != 0 else np.inf\n",
    "    \n",
    "    # 4. Calculate EVOL (Volume Error)\n",
    "    sum_observed = np.sum(observed)\n",
    "    sum_predicted = np.sum(predicted)\n",
    "    evol = np.abs(((sum_predicted - sum_observed) / sum_observed) * 100) if sum_observed != 0 else np.inf\n",
    "    \n",
    "    # 5. Calculate PE and TPE (Peak Error and Timing Peak Error)\n",
    "    threshold_obs = np.percentile(observed, 99) if len(observed) > 0 else 0\n",
    "    threshold_sim = np.percentile(predicted, 95) if len(predicted) > 0 else 0\n",
    "    \n",
    "    peaks_obs = find_peaks(observed, height=threshold_obs)[0] if len(observed) > 0 else []\n",
    "    peaks_sim = find_peaks(predicted, height=threshold_sim)[0] if len(predicted) > 0 else []\n",
    "    \n",
    "    tpe_values = []\n",
    "    pe_num = 0\n",
    "    pe_den = 0\n",
    "    \n",
    "    for peak in peaks_obs:\n",
    "        search_range = 5\n",
    "        start = max(0, peak - search_range)\n",
    "        end = min(len(predicted), peak + search_range)\n",
    "        lag = lead = search_range + 1\n",
    "        \n",
    "        if peak in peaks_sim:\n",
    "            tpe_values.append(0)\n",
    "            pe_num += np.abs(predicted[peak] - observed[peak])\n",
    "            pe_den += observed[peak]\n",
    "            continue\n",
    "            \n",
    "        # Find nearest peak in predicted\n",
    "        for i in range(start, end + 1):\n",
    "            if i in peaks_sim:\n",
    "                dist = i - peak\n",
    "                if dist < 0 and abs(dist) < lag:\n",
    "                    lag = abs(dist)\n",
    "                elif dist > 0 and dist < lead:\n",
    "                    lead = dist\n",
    "        \n",
    "        if lead <= search_range or lag <= search_range:\n",
    "            if lead < lag:\n",
    "                tpe_values.append(lead)\n",
    "                pe_idx = peak + lead\n",
    "            else:\n",
    "                tpe_values.append(-lag)\n",
    "                pe_idx = peak - lag\n",
    "                \n",
    "            if 0 <= pe_idx < len(predicted):\n",
    "                pe_num += np.abs(predicted[pe_idx] - observed[peak])\n",
    "                pe_den += observed[peak]\n",
    "    \n",
    "    # Calculate PE (Peak Error)\n",
    "    pe = (pe_num / pe_den) * 100 if pe_den != 0 else np.inf\n",
    "    \n",
    "    # Calculate average TPE (Timing Peak Error)\n",
    "    tpe = np.mean(np.abs(tpe_values)) if tpe_values else 0\n",
    "    \n",
    "    return nse, rsr, pbias, evol, pe, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Region: Hirakud | Training Data: 100%\n",
      "============================================================\n",
      "Test RMSE (Adjusted vs Raw): 729.4561\n",
      "\n",
      "Model Performance Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Step</th>\n",
       "      <th>NSE</th>\n",
       "      <th>RSR</th>\n",
       "      <th>PBIAS (%)</th>\n",
       "      <th>EVOL (%)</th>\n",
       "      <th>PE (%)</th>\n",
       "      <th>TPE (steps)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>0.0940</td>\n",
       "      <td>-3.1876</td>\n",
       "      <td>3.1876</td>\n",
       "      <td>4.9186</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.9836</td>\n",
       "      <td>0.1280</td>\n",
       "      <td>-4.6055</td>\n",
       "      <td>4.6055</td>\n",
       "      <td>6.3278</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>0.1702</td>\n",
       "      <td>-5.9989</td>\n",
       "      <td>5.9989</td>\n",
       "      <td>8.2136</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9471</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>-7.3123</td>\n",
       "      <td>7.3123</td>\n",
       "      <td>11.5455</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.9127</td>\n",
       "      <td>0.2955</td>\n",
       "      <td>-9.4370</td>\n",
       "      <td>9.4370</td>\n",
       "      <td>15.8103</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Step    NSE    RSR  PBIAS (%)  EVOL (%)  PE (%)  TPE (steps)\n",
       "0     1 0.9912 0.0940    -3.1876    3.1876  4.9186       0.0000\n",
       "1     2 0.9836 0.1280    -4.6055    4.6055  6.3278       0.0000\n",
       "2     3 0.9710 0.1702    -5.9989    5.9989  8.2136       0.0000\n",
       "3     4 0.9471 0.2300    -7.3123    7.3123 11.5455       0.0000\n",
       "4     5 0.9127 0.2955    -9.4370    9.4370 15.8103       0.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 8. PERFORMANCE EVALUATION ---\n",
    "# def calculate_metrics(observed, predicted):\n",
    "#     \"\"\"Calculate NSE, RSR, PBIAS, and TPE metrics.\"\"\"\n",
    "#     if np.sum(observed) == 0 or np.sum((observed - np.mean(observed)) ** 2) == 0:\n",
    "#         return -np.inf, np.inf, np.inf, np.inf\n",
    "    \n",
    "#     nse = 1 - (np.sum((observed - predicted) ** 2) / \n",
    "#                np.sum((observed - np.mean(observed)) ** 2))\n",
    "#     rsr = np.sqrt(np.sum((observed - predicted) ** 2)) / \\\n",
    "#           np.sqrt(np.sum((observed - np.mean(observed)) ** 2))\n",
    "#     pbias = (np.sum(predicted - observed) / np.sum(observed)) * 100\n",
    "#     tpe = (np.sum(np.abs(observed - predicted)) / np.sum(observed)) * 100\n",
    "    \n",
    "#     return nse, rsr, pbias, tpe\n",
    "\n",
    "# Calculate overall RMSE\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_raw, y_predict_adjusted))\n",
    "\n",
    "# Print header with region and training percentage\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Region: {REGION} | Training Data: {training_percentage}%\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test RMSE (Adjusted vs Raw): {test_rmse:.4f}\")\n",
    "\n",
    "# Calculate metrics for each prediction step\n",
    "metrics = [calculate_metrics(y_test_raw[:, i], y_predict_adjusted[:, i]) \n",
    "           for i in range(N_STEPS_OUT)]\n",
    "\n",
    "nse_values, rsr_values, pbias_values, evol_values, pe_values, tpe_values = zip(*metrics)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Step': range(1, len(nse_values) + 1),\n",
    "    'NSE': nse_values,\n",
    "    'RSR': rsr_values,\n",
    "    'PBIAS (%)': pbias_values,\n",
    "    'EVOL (%)': evol_values,\n",
    "    'PE (%)': pe_values,\n",
    "    'TPE (steps)': tpe_values\n",
    "})\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Display the metrics\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "display(metrics_df)\n",
    "\n",
    "\n",
    "# # Plot results\n",
    "# plt.figure(figsize=(18, 6))\n",
    "# step_to_plot = 0  # Plotting first prediction step (t+1)\n",
    "# plt.plot(y_test_raw[:, step_to_plot], 'b-', label='Actual (Raw)')\n",
    "# plt.plot(y_predict_adjusted[:, step_to_plot], 'r-', \n",
    "#          label='Predicted (Adjusted)', alpha=0.8)\n",
    "\n",
    "# plt.title(f'Discharge Prediction - Step t+{step_to_plot + 1}')\n",
    "# plt.xlabel(f'Sample Index (n={y_test_raw.shape[0]})')\n",
    "# plt.ylabel('Discharge')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7845267,
     "sourceId": 12437325,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7939697,
     "sourceId": 12572394,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959853,
     "sourceId": 12601992,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959857,
     "sourceId": 12601998,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959868,
     "sourceId": 12602014,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959881,
     "sourceId": 12602035,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8003950,
     "sourceId": 12665830,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
