{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available regions: 'Hirakud', 'Basantpur', 'Ghatora', 'Kurubhata', 'Rajim', 'Simga', 'Sundargarh'\n",
    "REGION = 'Hirakud'\n",
    "\n",
    "# Available percentage for training: 20, 50, 80, 100\n",
    "training_percentage = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Base paths\n",
    "DATA_DIR = Path('Data')\n",
    "MODEL_DIR = Path('models')\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model and training parameters\n",
    "N_STEPS_IN, N_STEPS_OUT = 30, 5 # Look back 30 days to predict next 5 days\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_w/4xfs9wld05nfzlbq4p03y6z80000gn/T/ipykernel_6957/3753173883.py:62: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data['Date'] = pd.to_datetime(data['Date'])\n"
     ]
    }
   ],
   "source": [
    "# Available regions: 'Hirakud', 'Basantpur', 'Ghatora', 'Kurubhata', 'Rajim', 'Simga', 'Sundargarh'\n",
    "# REGION = 'Kurubhata'\n",
    "\n",
    "# Base configuration\n",
    "BASE_FEATURES = {\n",
    "    'Hirakud': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Inflow', 'Sundargarh', 'Kurubhata'],\n",
    "        'target': 'Inflow'\n",
    "    },\n",
    "    'Basantpur': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Basantpur', 'Rajim', 'Simga', 'Ghatora'],\n",
    "        'target': 'Basantpur'\n",
    "    },\n",
    "    'Ghatora': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Ghatora'],\n",
    "        'target': 'Ghatora'\n",
    "    },\n",
    "    'Kurubhata': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Kurubhata'],\n",
    "        'target': 'Kurubhata'\n",
    "    },\n",
    "    'Rajim': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Rajim'],\n",
    "        'target': 'Rajim'\n",
    "    },\n",
    "    'Simga': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Simga'],\n",
    "        'target': 'Simga'\n",
    "    },\n",
    "    'Sundargarh': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Sundargarh'],\n",
    "        'target': 'Sundargarh'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate output columns dynamically\n",
    "def get_output_columns(features, region):\n",
    "    columns = {'Mean_areal_rainfall_upper': 'rainfall'}\n",
    "    for feat in features:\n",
    "        if feat != 'Mean_areal_rainfall_upper':\n",
    "            columns[feat] = feat.lower() + ('_discharge' if feat != 'rainfall' else '')\n",
    "    return columns\n",
    "\n",
    "# Build final config\n",
    "current_config = BASE_FEATURES.get(REGION)\n",
    "if not current_config:\n",
    "    raise ValueError(f\"Configuration for region '{REGION}' not found.\")\n",
    "\n",
    "# Set global variables\n",
    "FEATURES = current_config['features']\n",
    "TARGET_FEATURE = current_config['target']\n",
    "OUTPUT_COLUMNS = get_output_columns(FEATURES, REGION)\n",
    "TARGET_FEATURE_NAME = OUTPUT_COLUMNS[TARGET_FEATURE]\n",
    "FEATURES_TO_USE = list(OUTPUT_COLUMNS.values())\n",
    "MODEL_SAVE_PATH = f\"best_mlp_mann_model_{REGION}.pth\"\n",
    "RAW_TEST_FILE_PATH = f\"Data/raw_test_y_{REGION.lower()}.csv\"\n",
    "\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_excel('Data/new data.xlsx')\n",
    "data = data[['Date'] + FEATURES].rename(columns=OUTPUT_COLUMNS)\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.set_index('Date').ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(data, training_percentage=100):\n",
    "    # Define the split points based on percentage\n",
    "    split_dates = {\n",
    "        100: '2000-01-01',  # 100% of data -> 4017 days\n",
    "        80: '2002-01-01',   # 80% of data -> 3286 days\n",
    "        50: '2005-01-01',    # 50% of data -> 2190 days\n",
    "        20: '2008-01-01'\n",
    "    }\n",
    "    \n",
    "    if training_percentage not in split_dates:\n",
    "        raise ValueError(\"training_percentage must be one of: 50, 80, or 100\")      # Would like to change to a continous form\n",
    "    \n",
    "    # Split the data\n",
    "    train_start = split_dates[training_percentage]\n",
    "    data_train = data.loc[train_start:'2010-12-31'].copy()\n",
    "    data_test = data.loc['2011-01-01':'2014-12-31'].copy()\n",
    "    \n",
    "    return data_train, data_test\n",
    "\n",
    "# usage:\n",
    "# training_percentage = 50\n",
    "data_train, data_test = get_train_test_split(data, training_percentage)\n",
    "\n",
    "# Keep raw test target before processing\n",
    "raw_data_test_target = data_test[[TARGET_FEATURE_NAME]].copy()\n",
    "\n",
    "# Smooth features\n",
    "for col in FEATURES_TO_USE:\n",
    "    window = np.blackman(20)\n",
    "    data_train[col] = np.convolve(window/window.sum(), data_train[col].values, 'same')\n",
    "    data_test[col] = np.convolve(window/window.sum(), data_test[col].values, 'same')\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(data_train)\n",
    "train_scaled = scaler.transform(data_train)\n",
    "test_scaled = scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the target column in the feature set\n",
    "target_col_index = FEATURES_TO_USE.index(TARGET_FEATURE_NAME)\n",
    "\n",
    "def create_sequences(data, n_steps_in, n_steps_out, target_idx):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps_in - n_steps_out + 1):\n",
    "        X.append(data[i:i+n_steps_in])\n",
    "        y.append(data[i+n_steps_in:i+n_steps_in+n_steps_out, target_idx])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(train_scaled, N_STEPS_IN, N_STEPS_OUT, target_col_index)\n",
    "\n",
    "# Split into train/validation\n",
    "train_X, val_X, train_y, val_y = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_data = TensorDataset(\n",
    "    torch.tensor(train_X, dtype=torch.float32),\n",
    "    torch.tensor(train_y, dtype=torch.float32)\n",
    ")\n",
    "val_data = TensorDataset(\n",
    "    torch.tensor(val_X, dtype=torch.float32),\n",
    "    torch.tensor(val_y, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. MODEL DEFINITION (MLP+MANN) ---\n",
    "class MLP_MANN_Model(nn.Module):\n",
    "    def __init__(self, input_dim, n_steps_in, n_steps_out, memory_size=128, memory_dim=64, hidden_dim=256):\n",
    "        super(MLP_MANN_Model, self).__init__()\n",
    "        flattened_input_dim = n_steps_in * input_dim\n",
    "        self.memory = nn.Parameter(torch.randn(memory_size, memory_dim))\n",
    "        nn.init.xavier_uniform_(self.memory)\n",
    "        self.input_projection = nn.Linear(flattened_input_dim, memory_dim)\n",
    "        self.dense1 = nn.Linear(flattened_input_dim + memory_dim, hidden_dim)\n",
    "        self.dense2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.output_layer = nn.Linear(hidden_dim // 2, n_steps_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        flat_x = x.view(x.size(0), -1)\n",
    "        projected_input = self.input_projection(flat_x)\n",
    "        attention_scores = torch.matmul(projected_input, self.memory.t())\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        memory_output = torch.matmul(attention_weights, self.memory)\n",
    "        augmented_input = torch.cat([flat_x, memory_output], dim=1)\n",
    "        h1 = self.relu(self.dense1(augmented_input))\n",
    "        h2 = self.relu(self.dense2(h1))\n",
    "        output = self.output_layer(h2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, batch_X, batch_y, criterion, optimizer, device):\n",
    "    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(batch_X)\n",
    "    loss = criterion(outputs, batch_y)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for X_batch, y_batch in data_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        total_loss += criterion(outputs, y_batch).item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, criterion, optimizer, device, model_path):\n",
    "    model = model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train': [], 'val': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            train_loss += train_step(model, batch_X, batch_y, criterion, optimizer, device)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "        # Record history\n",
    "        history['train'].append(train_loss)\n",
    "        history['val'].append(val_loss)\n",
    "        \n",
    "        # print(f\"Epoch {epoch+1:3d}/{epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# Training setup\n",
    "input_dim = X.shape[-1]\n",
    "model = MLP_MANN_Model(input_dim, N_STEPS_IN, N_STEPS_OUT)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create model path with region name\n",
    "model_path = MODEL_DIR / f\"best_mlp_mann_model_{REGION}_{training_percentage}pct.pth\"\n",
    "\n",
    "# Train the model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    model_path=str(model_path)  # Convert Path to string for PyTorch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 6. POST-TRAINING ANALYSIS ---\n",
    "# print(\"\\n--- 6. Plotting Training and Validation Loss ---\")\n",
    "# if history:  # Check if history exists\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.plot(history['train'], label='Train Loss', color='blue')\n",
    "#     plt.plot(history['val'], label='Validation Loss', color='orange')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Loss (MSE)')\n",
    "#     plt.title('MLP+MANN (Hirakud): Training and Validation Loss', fontsize=16)\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. FINAL EVALUATION ---\n",
    "# Create test sequences\n",
    "X_test, y_test_scaled = create_sequences(test_scaled, N_STEPS_IN, N_STEPS_OUT, target_col_index)\n",
    "# print(f\"Testing on {X_test.shape[0]} samples\")\n",
    "\n",
    "# Load best model from the models directory\n",
    "model_path = MODEL_DIR / f\"best_mlp_mann_model_{REGION}.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device).eval()\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    test_predictions_scaled = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Inverse scaling function\n",
    "def inverse_scale_data(scaled_data, original_scaler, n_features, target_idx):\n",
    "    dummy = np.zeros((scaled_data.size, n_features))\n",
    "    dummy[:, target_idx] = scaled_data.flatten()\n",
    "    return original_scaler.inverse_transform(dummy)[:, target_idx].reshape(scaled_data.shape)\n",
    "\n",
    "# Apply inverse scaling\n",
    "n_features = train_scaled.shape[1]\n",
    "y_test_inv = inverse_scale_data(y_test_scaled, scaler, n_features, target_col_index)\n",
    "test_predictions_inv = inverse_scale_data(test_predictions_scaled, scaler, n_features, target_col_index)\n",
    "\n",
    "# Load raw test data\n",
    "raw_y_test = pd.read_csv(RAW_TEST_FILE_PATH, header=0, dtype=float).values\n",
    "num_samples = raw_y_test.shape[0]\n",
    "# print(f\"Loaded raw test data with {num_samples} samples\")\n",
    "\n",
    "# Align and calculate offset\n",
    "y_test_raw = raw_y_test[:num_samples]\n",
    "y_test_smoothed = y_test_inv[:num_samples]\n",
    "y_predict_smoothed = test_predictions_inv[:num_samples]\n",
    "\n",
    "# Apply offset\n",
    "offset = y_test_raw - y_test_smoothed\n",
    "y_predict_adjusted = y_predict_smoothed + offset\n",
    "# print(\"Offset calculation and adjustment complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Region: Hirakud | Training Data: 100%\n",
      "============================================================\n",
      "Test RMSE (Adjusted vs Raw): 912.3874\n",
      "\n",
      "Metrics per prediction step (t+1 to t+5):\n",
      "NSE:    ['0.9821', '0.9695', '0.9515', '0.9168', '0.8760']\n",
      "RSR:    ['0.1340', '0.1747', '0.2202', '0.2884', '0.3521']\n",
      "PBIAS:  ['-4.97%', '-6.41%', '-6.74%', '-8.03%', '-9.38%']\n",
      "TPE:    ['10.31%', '13.89%', '18.17%', '23.04%', '27.84%']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- 8. PERFORMANCE EVALUATION ---\n",
    "def calculate_metrics(observed, predicted):\n",
    "    \"\"\"Calculate NSE, RSR, PBIAS, and TPE metrics.\"\"\"\n",
    "    if np.sum(observed) == 0 or np.sum((observed - np.mean(observed)) ** 2) == 0:\n",
    "        return -np.inf, np.inf, np.inf, np.inf\n",
    "    \n",
    "    nse = 1 - (np.sum((observed - predicted) ** 2) / \n",
    "               np.sum((observed - np.mean(observed)) ** 2))\n",
    "    rsr = np.sqrt(np.sum((observed - predicted) ** 2)) / \\\n",
    "          np.sqrt(np.sum((observed - np.mean(observed)) ** 2))\n",
    "    pbias = (np.sum(predicted - observed) / np.sum(observed)) * 100\n",
    "    tpe = (np.sum(np.abs(observed - predicted)) / np.sum(observed)) * 100\n",
    "    \n",
    "    return nse, rsr, pbias, tpe\n",
    "\n",
    "# Calculate overall RMSE\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_raw, y_predict_adjusted))\n",
    "\n",
    "# Print header with region and training percentage\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Region: {REGION} | Training Data: {training_percentage}%\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test RMSE (Adjusted vs Raw): {test_rmse:.4f}\")\n",
    "\n",
    "# Calculate metrics for each prediction step\n",
    "metrics = [calculate_metrics(y_test_raw[:, i], y_predict_adjusted[:, i]) \n",
    "           for i in range(N_STEPS_OUT)]\n",
    "nse_values, rsr_values, pbias_values, tpe_values = zip(*metrics)\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nMetrics per prediction step (t+1 to t+5):\")\n",
    "print(f\"NSE:    {[f'{x:.4f}' for x in nse_values]}\")\n",
    "print(f\"RSR:    {[f'{x:.4f}' for x in rsr_values]}\")\n",
    "print(f\"PBIAS:  {[f'{x:.2f}%' for x in pbias_values]}\")\n",
    "print(f\"TPE:    {[f'{x:.2f}%' for x in tpe_values]}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# # Plot results\n",
    "# plt.figure(figsize=(18, 6))\n",
    "# step_to_plot = 0  # Plotting first prediction step (t+1)\n",
    "# plt.plot(y_test_raw[:, step_to_plot], 'b-', label='Actual (Raw)')\n",
    "# plt.plot(y_predict_adjusted[:, step_to_plot], 'r-', \n",
    "#          label='Predicted (Adjusted)', alpha=0.8)\n",
    "\n",
    "# plt.title(f'Discharge Prediction - Step t+{step_to_plot + 1}')\n",
    "# plt.xlabel(f'Sample Index (n={y_test_raw.shape[0]})')\n",
    "# plt.ylabel('Discharge')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7845267,
     "sourceId": 12437325,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7939697,
     "sourceId": 12572394,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959853,
     "sourceId": 12601992,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959857,
     "sourceId": 12601998,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959868,
     "sourceId": 12602014,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959881,
     "sourceId": 12602035,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8003950,
     "sourceId": 12665830,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
