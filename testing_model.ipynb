{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available regions: 'Hirakud', 'Basantpur', 'Ghatora', 'Kurubhata', 'Rajim', 'Simga', 'Sundargarh'\n",
    "REGION = 'Hirakud'\n",
    "\n",
    "# Available percentage for training: 20, 50, 80, 100\n",
    "training_percentage = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Base paths\n",
    "DATA_DIR = Path('Data')\n",
    "MODEL_DIR = Path('models')\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model and training parameters\n",
    "N_STEPS_IN, N_STEPS_OUT = 30, 5 # Look back 30 days to predict next 5 days\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available regions: 'Hirakud', 'Basantpur', 'Ghatora', 'Kurubhata', 'Rajim', 'Simga', 'Sundargarh'\n",
    "# REGION = 'Kurubhata'\n",
    "\n",
    "# Base configuration\n",
    "BASE_FEATURES = {\n",
    "    'Hirakud': {        # using different dataset\n",
    "        'features': [\"Simga\", \"Rajim\", \"Rampur\", \"Andhiyarkhore\", \"Ghatora\", \"Bamnidhi\", \"Kurubhata\", \"Kelo\", \"Sundargarh\", \"Paramanpur\", \"Basantpur\", \"Hirakud\", \"Mean_areal_rainfall_upper\"],\n",
    "        'target': 'Hirakud'\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_output_columns(features, region):\n",
    "    columns = {}\n",
    "    for feat in features:\n",
    "        if feat == 'Mean_areal_rainfall_upper':\n",
    "            columns[feat] = 'rainfall'\n",
    "        else:\n",
    "            # For all other features, convert to lowercase and add _discharge\n",
    "            columns[feat] = feat.lower() + '_discharge'\n",
    "    return columns\n",
    "\n",
    "# Build final config\n",
    "current_config = BASE_FEATURES.get(REGION)\n",
    "if not current_config:\n",
    "    raise ValueError(f\"Configuration for region '{REGION}' not found.\")\n",
    "\n",
    "# Set global variables\n",
    "FEATURES = current_config['features']\n",
    "TARGET_FEATURE = current_config['target']\n",
    "OUTPUT_COLUMNS = get_output_columns(FEATURES, REGION)\n",
    "TARGET_FEATURE_NAME = OUTPUT_COLUMNS[TARGET_FEATURE]\n",
    "FEATURES_TO_USE = list(OUTPUT_COLUMNS.values())\n",
    "MODEL_SAVE_PATH = f\"best_mlp_mann_model_{REGION}.pth\"\n",
    "RAW_TEST_FILE_PATH = f\"Data/raw_test_y_{REGION.lower()}.csv\"\n",
    "\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv('Data/discharge_1995_2019.csv')\n",
    "data = data.rename(columns=OUTPUT_COLUMNS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(data, training_percentage=100, target_feature='target', features_to_use=None):\n",
    "    \n",
    "    # Set default features if not provided\n",
    "    if features_to_use is None:\n",
    "        features_to_use = [col for col in data.columns if col != target_feature]\n",
    "    \n",
    "    # Calculate number of training rows based on percentage\n",
    "    total_train_rows = 2000\n",
    "    train_rows = int((training_percentage / 100) * total_train_rows)\n",
    "    \n",
    "    # Get test data (fixed from row 2000 to end)\n",
    "    data_test = data.iloc[1999:].copy()\n",
    "    \n",
    "    # Get training data (adjusts based on percentage)\n",
    "    if training_percentage == 100:\n",
    "        data_train = data.iloc[:2000].copy()\n",
    "    else:\n",
    "        # Remove initial rows to reduce training data size\n",
    "        start_row = 2000 - train_rows\n",
    "        data_train = data.iloc[start_row:2000].copy()\n",
    "    \n",
    "    # Keep raw test target before processing\n",
    "    raw_data_test_target = data_test[[target_feature]].copy()\n",
    "    \n",
    "    # Smooth features\n",
    "    for col in features_to_use:\n",
    "        window = np.blackman(20)\n",
    "        window /= window.sum()\n",
    "        data_train[col] = np.convolve(data_train[col].values, window, 'same')\n",
    "        data_test[col] = np.convolve(data_test[col].values, window, 'same')\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = scaler.fit_transform(data_train[features_to_use])\n",
    "    test_scaled = scaler.transform(data_test[features_to_use])\n",
    "    \n",
    "    return train_scaled, test_scaled, raw_data_test_target, scaler\n",
    "\n",
    "# Usage:\n",
    "# training_percentage = 50\n",
    "target_feature = TARGET_FEATURE_NAME\n",
    "features_to_use = FEATURES_TO_USE  # or None to use all except target\n",
    "train_scaled, test_scaled, raw_data_test_target, scaler = get_train_test_data(\n",
    "    data, training_percentage, target_feature, features_to_use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_scaled.shape, test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_train_test_split(data, training_percentage=100):\n",
    "#     # Define the split points based on percentage\n",
    "#     split_dates = {\n",
    "#         100: '2000-01-01',  # 100% of data -> 4017 days\n",
    "#         80: '2002-01-01',   # 80% of data -> 3286 days\n",
    "#         50: '2005-01-01',    # 50% of data -> 2190 days\n",
    "#         20: '2008-01-01'\n",
    "#     }\n",
    "    \n",
    "#     if training_percentage not in split_dates:\n",
    "#         raise ValueError(\"training_percentage must be one of: 50, 80, or 100\")      # Would like to change to a continous form\n",
    "    \n",
    "#     # Split the data\n",
    "#     train_start = split_dates[training_percentage]\n",
    "#     data_train = data.loc[train_start:'2010-12-31'].copy()\n",
    "#     data_test = data.loc['2011-01-01':'2014-12-31'].copy()\n",
    "    \n",
    "#     return data_train, data_test\n",
    "\n",
    "# # usage:\n",
    "# # training_percentage = 50\n",
    "# data_train, data_test = get_train_test_split(data, training_percentage)\n",
    "\n",
    "# # Keep raw test target before processing\n",
    "# raw_data_test_target = data_test[[TARGET_FEATURE_NAME]].copy()\n",
    "\n",
    "# # Smooth features\n",
    "# for col in FEATURES_TO_USE:\n",
    "#     window = np.blackman(20)\n",
    "#     data_train[col] = np.convolve(window/window.sum(), data_train[col].values, 'same')\n",
    "#     data_test[col] = np.convolve(window/window.sum(), data_test[col].values, 'same')\n",
    "\n",
    "# # Scale data\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaler.fit(data_train)\n",
    "# train_scaled = scaler.transform(data_train)\n",
    "# test_scaled = scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the target column in the feature set\n",
    "target_col_index = FEATURES_TO_USE.index(TARGET_FEATURE_NAME)\n",
    "\n",
    "def create_sequences(data, n_steps_in, n_steps_out, target_idx):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps_in - n_steps_out + 1):\n",
    "        X.append(data[i:i+n_steps_in])\n",
    "        y.append(data[i+n_steps_in:i+n_steps_in+n_steps_out, target_idx])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(train_scaled, N_STEPS_IN, N_STEPS_OUT, target_col_index)\n",
    "\n",
    "# Split into train/validation\n",
    "train_X, val_X, train_y, val_y = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_data = TensorDataset(\n",
    "    torch.tensor(train_X, dtype=torch.float32),\n",
    "    torch.tensor(train_y, dtype=torch.float32)\n",
    ")\n",
    "val_data = TensorDataset(\n",
    "    torch.tensor(val_X, dtype=torch.float32),\n",
    "    torch.tensor(val_y, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. MODEL DEFINITION (MLP+MANN) ---\n",
    "class MLP_MANN_Model(nn.Module):\n",
    "    def __init__(self, input_dim, n_steps_in, n_steps_out, memory_size=128, memory_dim=64, hidden_dim=256):\n",
    "        super(MLP_MANN_Model, self).__init__()\n",
    "        flattened_input_dim = n_steps_in * input_dim\n",
    "        self.memory = nn.Parameter(torch.randn(memory_size, memory_dim))\n",
    "        nn.init.xavier_uniform_(self.memory)\n",
    "        self.input_projection = nn.Linear(flattened_input_dim, memory_dim)\n",
    "        self.dense1 = nn.Linear(flattened_input_dim + memory_dim, hidden_dim)\n",
    "        self.dense2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.output_layer = nn.Linear(hidden_dim // 2, n_steps_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        flat_x = x.view(x.size(0), -1)\n",
    "        projected_input = self.input_projection(flat_x)\n",
    "        attention_scores = torch.matmul(projected_input, self.memory.t())\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        memory_output = torch.matmul(attention_weights, self.memory)\n",
    "        augmented_input = torch.cat([flat_x, memory_output], dim=1)\n",
    "        h1 = self.relu(self.dense1(augmented_input))\n",
    "        h2 = self.relu(self.dense2(h1))\n",
    "        output = self.output_layer(h2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, batch_X, batch_y, criterion, optimizer, device):\n",
    "    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(batch_X)\n",
    "    loss = criterion(outputs, batch_y)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for X_batch, y_batch in data_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        total_loss += criterion(outputs, y_batch).item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, criterion, optimizer, device, model_path, save_model=True):\n",
    "    model = model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train': [], 'val': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            train_loss += train_step(model, batch_X, batch_y, criterion, optimizer, device)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Save best model if save_model is True\n",
    "        if save_model and val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "        # Record history\n",
    "        history['train'].append(train_loss)\n",
    "        history['val'].append(val_loss)\n",
    "        \n",
    "        # print(f\"Epoch {epoch+1:3d}/{epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# Training setup\n",
    "input_dim = X.shape[-1]\n",
    "model = MLP_MANN_Model(input_dim, N_STEPS_IN, N_STEPS_OUT)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create model path with region name\n",
    "model_path = MODEL_DIR / f\"best_mlp_mann_model_{REGION}_{training_percentage}pct.pth\"\n",
    "\n",
    "# Train the model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    model_path=str(model_path),\n",
    "    save_model=True  # Set to False to disable model saving\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 6. POST-TRAINING ANALYSIS ---\n",
    "# print(\"\\n--- 6. Plotting Training and Validation Loss ---\")\n",
    "# if history:  # Check if history exists\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.plot(history['train'], label='Train Loss', color='blue')\n",
    "#     plt.plot(history['val'], label='Validation Loss', color='orange')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Loss (MSE)')\n",
    "#     plt.title('MLP+MANN (Hirakud): Training and Validation Loss', fontsize=16)\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. FINAL EVALUATION ---\n",
    "# Create test sequences\n",
    "X_test, y_test_scaled = create_sequences(test_scaled, N_STEPS_IN, N_STEPS_OUT, target_col_index)\n",
    "# print(f\"Testing on {X_test.shape[0]} samples\")\n",
    "\n",
    "# Load best model from the models directory\n",
    "model_path = MODEL_DIR / f\"best_mlp_mann_model_{REGION}_{training_percentage}pct.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device).eval()\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    test_predictions_scaled = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Inverse scaling function\n",
    "def inverse_scale_data(scaled_data, original_scaler, n_features, target_idx):\n",
    "    dummy = np.zeros((scaled_data.size, n_features))\n",
    "    dummy[:, target_idx] = scaled_data.flatten()\n",
    "    return original_scaler.inverse_transform(dummy)[:, target_idx].reshape(scaled_data.shape)\n",
    "\n",
    "# Apply inverse scaling\n",
    "n_features = train_scaled.shape[1]\n",
    "y_test_inv = inverse_scale_data(y_test_scaled, scaler, n_features, target_col_index)\n",
    "test_predictions_inv = inverse_scale_data(test_predictions_scaled, scaler, n_features, target_col_index)\n",
    "\n",
    "# Load raw test data\n",
    "raw_y_test = pd.read_csv(RAW_TEST_FILE_PATH, header=0, dtype=float).values\n",
    "num_samples = raw_y_test.shape[0]\n",
    "# print(f\"Loaded raw test data with {num_samples} samples\")\n",
    "\n",
    "# Align and calculate offset\n",
    "y_test_raw = raw_y_test[:num_samples]\n",
    "y_test_smoothed = y_test_inv[:num_samples]\n",
    "y_predict_smoothed = test_predictions_inv[:num_samples]\n",
    "\n",
    "# Apply offset\n",
    "offset = y_test_raw - y_test_smoothed\n",
    "y_predict_adjusted = y_predict_smoothed + offset\n",
    "# print(\"Offset calculation and adjustment complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(observed, predicted):\n",
    "    \"\"\"Calculate NSE, RSR, PBIAS, EVOL, PE, and TPE metrics.\n",
    "    \n",
    "    Args:\n",
    "        observed: Array of observed values\n",
    "        predicted: Array of predicted values\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (NSE, RSR, PBIAS, EVOL, PE, TPE)\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if len(observed) == 0 or len(predicted) == 0 or len(observed) != len(predicted):\n",
    "        return -np.inf, np.inf, np.inf, np.inf, np.inf, np.inf\n",
    "    \n",
    "    # 1. Calculate NSE (Nash-Sutcliffe Efficiency)\n",
    "    observed_mean = np.mean(observed)\n",
    "    numerator = np.sum((observed - predicted) ** 2)\n",
    "    denominator = np.sum((observed - observed_mean) ** 2)\n",
    "    nse = 1 - (numerator / denominator) if denominator != 0 else -np.inf\n",
    "    \n",
    "    # 2. Calculate RSR (Root Mean Square Error - Observations Standard deviation Ratio)\n",
    "    rsr_numerator = np.sqrt(np.sum((observed - predicted) ** 2))\n",
    "    rsr_denominator = np.sqrt(np.sum((observed - observed_mean) ** 2))\n",
    "    rsr = rsr_numerator / rsr_denominator if rsr_denominator != 0 else np.inf\n",
    "    \n",
    "    # 3. Calculate PBIAS (Percent Bias)\n",
    "    pbias = (np.sum(predicted - observed) / np.sum(observed)) * 100 if np.sum(observed) != 0 else np.inf\n",
    "    \n",
    "    # 4. Calculate EVOL (Volume Error)\n",
    "    sum_observed = np.sum(observed)\n",
    "    sum_predicted = np.sum(predicted)\n",
    "    evol = np.abs(((sum_predicted - sum_observed) / sum_observed) * 100) if sum_observed != 0 else np.inf\n",
    "    \n",
    "    # 5. Calculate PE and TPE (Peak Error and Timing Peak Error)\n",
    "    threshold_obs = np.percentile(observed, 99) if len(observed) > 0 else 0\n",
    "    threshold_sim = np.percentile(predicted, 95) if len(predicted) > 0 else 0\n",
    "    \n",
    "    peaks_obs = find_peaks(observed, height=threshold_obs)[0] if len(observed) > 0 else []\n",
    "    peaks_sim = find_peaks(predicted, height=threshold_sim)[0] if len(predicted) > 0 else []\n",
    "    \n",
    "    tpe_values = []\n",
    "    pe_num = 0\n",
    "    pe_den = 0\n",
    "    \n",
    "    for peak in peaks_obs:\n",
    "        search_range = 5\n",
    "        start = max(0, peak - search_range)\n",
    "        end = min(len(predicted), peak + search_range)\n",
    "        lag = lead = search_range + 1\n",
    "        \n",
    "        if peak in peaks_sim:\n",
    "            tpe_values.append(0)\n",
    "            pe_num += np.abs(predicted[peak] - observed[peak])\n",
    "            pe_den += observed[peak]\n",
    "            continue\n",
    "            \n",
    "        # Find nearest peak in predicted\n",
    "        for i in range(start, end + 1):\n",
    "            if i in peaks_sim:\n",
    "                dist = i - peak\n",
    "                if dist < 0 and abs(dist) < lag:\n",
    "                    lag = abs(dist)\n",
    "                elif dist > 0 and dist < lead:\n",
    "                    lead = dist\n",
    "        \n",
    "        if lead <= search_range or lag <= search_range:\n",
    "            if lead < lag:\n",
    "                tpe_values.append(lead)\n",
    "                pe_idx = peak + lead\n",
    "            else:\n",
    "                tpe_values.append(-lag)\n",
    "                pe_idx = peak - lag\n",
    "                \n",
    "            if 0 <= pe_idx < len(predicted):\n",
    "                pe_num += np.abs(predicted[pe_idx] - observed[peak])\n",
    "                pe_den += observed[peak]\n",
    "    \n",
    "    # Calculate PE (Peak Error)\n",
    "    pe = (pe_num / pe_den) * 100 if pe_den != 0 else np.inf\n",
    "    \n",
    "    # Calculate average TPE (Timing Peak Error)\n",
    "    tpe = np.mean(np.abs(tpe_values)) if tpe_values else 0\n",
    "    \n",
    "    return nse, rsr, pbias, evol, pe, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Region: Hirakud | Training Data: 20%\n",
      "============================================================\n",
      "Test RMSE (Adjusted vs Raw): 2097.7160\n",
      "\n",
      "Model Performance Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Step</th>\n",
       "      <th>NSE</th>\n",
       "      <th>RSR</th>\n",
       "      <th>PBIAS (%)</th>\n",
       "      <th>EVOL (%)</th>\n",
       "      <th>PE (%)</th>\n",
       "      <th>TPE (steps)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.8409</td>\n",
       "      <td>0.3989</td>\n",
       "      <td>14.2207</td>\n",
       "      <td>14.2207</td>\n",
       "      <td>3.6121</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.7749</td>\n",
       "      <td>0.4744</td>\n",
       "      <td>15.4067</td>\n",
       "      <td>15.4067</td>\n",
       "      <td>4.1368</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.6569</td>\n",
       "      <td>0.5858</td>\n",
       "      <td>18.1517</td>\n",
       "      <td>18.1517</td>\n",
       "      <td>4.5973</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.6395</td>\n",
       "      <td>15.9035</td>\n",
       "      <td>15.9035</td>\n",
       "      <td>4.4237</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5293</td>\n",
       "      <td>0.6861</td>\n",
       "      <td>6.7535</td>\n",
       "      <td>6.7535</td>\n",
       "      <td>4.2387</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Step    NSE    RSR  PBIAS (%)  EVOL (%)  PE (%)  TPE (steps)\n",
       "0     1 0.8409 0.3989    14.2207   14.2207  3.6121       0.0000\n",
       "1     2 0.7749 0.4744    15.4067   15.4067  4.1368       0.0000\n",
       "2     3 0.6569 0.5858    18.1517   18.1517  4.5973       0.0000\n",
       "3     4 0.5910 0.6395    15.9035   15.9035  4.4237       0.0000\n",
       "4     5 0.5293 0.6861     6.7535    6.7535  4.2387       0.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 8. PERFORMANCE EVALUATION ---\n",
    "# def calculate_metrics(observed, predicted):\n",
    "#     \"\"\"Calculate NSE, RSR, PBIAS, and TPE metrics.\"\"\"\n",
    "#     if np.sum(observed) == 0 or np.sum((observed - np.mean(observed)) ** 2) == 0:\n",
    "#         return -np.inf, np.inf, np.inf, np.inf\n",
    "    \n",
    "#     nse = 1 - (np.sum((observed - predicted) ** 2) / \n",
    "#                np.sum((observed - np.mean(observed)) ** 2))\n",
    "#     rsr = np.sqrt(np.sum((observed - predicted) ** 2)) / \\\n",
    "#           np.sqrt(np.sum((observed - np.mean(observed)) ** 2))\n",
    "#     pbias = (np.sum(predicted - observed) / np.sum(observed)) * 100\n",
    "#     tpe = (np.sum(np.abs(observed - predicted)) / np.sum(observed)) * 100\n",
    "    \n",
    "#     return nse, rsr, pbias, tpe\n",
    "\n",
    "# Calculate overall RMSE\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_raw, y_predict_adjusted))\n",
    "\n",
    "# Print header with region and training percentage\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Region: {REGION} | Training Data: {training_percentage}%\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test RMSE (Adjusted vs Raw): {test_rmse:.4f}\")\n",
    "\n",
    "# Calculate metrics for each prediction step\n",
    "metrics = [calculate_metrics(y_test_raw[:, i], y_predict_adjusted[:, i]) \n",
    "           for i in range(N_STEPS_OUT)]\n",
    "\n",
    "nse_values, rsr_values, pbias_values, evol_values, pe_values, tpe_values = zip(*metrics)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Step': range(1, len(nse_values) + 1),\n",
    "    'NSE': nse_values,\n",
    "    'RSR': rsr_values,\n",
    "    'PBIAS (%)': pbias_values,\n",
    "    'EVOL (%)': evol_values,\n",
    "    'PE (%)': pe_values,\n",
    "    'TPE (steps)': tpe_values\n",
    "})\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Display the metrics\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "display(metrics_df)\n",
    "\n",
    "\n",
    "# # Plot results\n",
    "# plt.figure(figsize=(18, 6))\n",
    "# step_to_plot = 0  # Plotting first prediction step (t+1)\n",
    "# plt.plot(y_test_raw[:, step_to_plot], 'b-', label='Actual (Raw)')\n",
    "# plt.plot(y_predict_adjusted[:, step_to_plot], 'r-', \n",
    "#          label='Predicted (Adjusted)', alpha=0.8)\n",
    "\n",
    "# plt.title(f'Discharge Prediction - Step t+{step_to_plot + 1}')\n",
    "# plt.xlabel(f'Sample Index (n={y_test_raw.shape[0]})')\n",
    "# plt.ylabel('Discharge')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7845267,
     "sourceId": 12437325,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7939697,
     "sourceId": 12572394,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959853,
     "sourceId": 12601992,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959857,
     "sourceId": 12601998,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959868,
     "sourceId": 12602014,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959881,
     "sourceId": 12602035,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8003950,
     "sourceId": 12665830,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
