{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available regions: 'Hirakud', 'Basantpur', 'Ghatora', 'Kurubhata', 'Rajim', 'Simga', 'Sundargarh'\n",
    "REGION = 'Hirakud'\n",
    "\n",
    "# Available percentage for training: 20, 50, 80, 100\n",
    "training_percentage = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Base paths\n",
    "DATA_DIR = Path('Data')\n",
    "MODEL_DIR = Path('models')\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model and training parameters\n",
    "N_STEPS_IN, N_STEPS_OUT = 30, 5 # Look back 30 days to predict next 5 days\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available regions: 'Hirakud', 'Basantpur', 'Ghatora', 'Kurubhata', 'Rajim', 'Simga', 'Sundargarh'\n",
    "# REGION = 'Kurubhata'\n",
    "\n",
    "# Base configuration\n",
    "BASE_FEATURES = {\n",
    "    # 'Hirakud': {\n",
    "    #     'features': ['Mean_areal_rainfall_upper', 'Inflow', 'Sundargarh', 'Kurubhata'],\n",
    "    #     'target': 'Inflow'\n",
    "    # },\n",
    "    'Hirakud': {        # using different dataset\n",
    "        'features': [\"Simga\", \"Rajim\", \"Rampur\", \"Andhiyarkhore\", \"Ghatora\", \"Bamnidhi\", \"Kurubhata\", \"Kelo\", \"Sundargarh\", \"Paramanpur\", \"Basantpur\", \"Hirakud\", \"Mean_areal_rainfall_upper\"],\n",
    "        'target': 'Hirakud'\n",
    "    },\n",
    "    'Basantpur': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Basantpur', 'Rajim', 'Simga', 'Ghatora'],\n",
    "        'target': 'Basantpur'\n",
    "    },\n",
    "    'Ghatora': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Ghatora'],\n",
    "        'target': 'Ghatora'\n",
    "    },\n",
    "    'Kurubhata': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Kurubhata'],\n",
    "        'target': 'Kurubhata'\n",
    "    },\n",
    "    'Rajim': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Rajim'],\n",
    "        'target': 'Rajim'\n",
    "    },\n",
    "    'Simga': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Simga'],\n",
    "        'target': 'Simga'\n",
    "    },\n",
    "    'Sundargarh': {\n",
    "        'features': ['Mean_areal_rainfall_upper', 'Sundargarh'],\n",
    "        'target': 'Sundargarh'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate output columns dynamically\n",
    "def get_output_columns(features, region):\n",
    "    columns = {'Mean_areal_rainfall_upper': 'rainfall'}\n",
    "    for feat in features:\n",
    "        if feat != 'Mean_areal_rainfall_upper':\n",
    "            columns[feat] = feat.lower() + ('_discharge' if feat != 'rainfall' else '')\n",
    "    return columns\n",
    "\n",
    "# Build final config\n",
    "current_config = BASE_FEATURES.get(REGION)\n",
    "if not current_config:\n",
    "    raise ValueError(f\"Configuration for region '{REGION}' not found.\")\n",
    "\n",
    "# Set global variables\n",
    "FEATURES = current_config['features']\n",
    "TARGET_FEATURE = current_config['target']\n",
    "OUTPUT_COLUMNS = get_output_columns(FEATURES, REGION)\n",
    "TARGET_FEATURE_NAME = OUTPUT_COLUMNS[TARGET_FEATURE]\n",
    "FEATURES_TO_USE = list(OUTPUT_COLUMNS.values())\n",
    "MODEL_SAVE_PATH = f\"best_mlp_mann_model_{REGION}.pth\"\n",
    "RAW_TEST_FILE_PATH = f\"Data/raw_test_y_{REGION.lower()}.csv\"\n",
    "\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv('Data/discharge_1995_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Simga</th>\n",
       "      <th>Rajim</th>\n",
       "      <th>Rampur</th>\n",
       "      <th>Andhiyarkhore</th>\n",
       "      <th>Ghatora</th>\n",
       "      <th>Bamnidhi</th>\n",
       "      <th>Kurubhata</th>\n",
       "      <th>Kelo</th>\n",
       "      <th>Sundargarh</th>\n",
       "      <th>Paramanpur</th>\n",
       "      <th>Basantpur</th>\n",
       "      <th>Hirakud</th>\n",
       "      <th>Mean_areal_rainfall_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.70</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.651</td>\n",
       "      <td>1.155</td>\n",
       "      <td>71.48</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.50</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.675</td>\n",
       "      <td>1.183</td>\n",
       "      <td>53.11</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.17</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664</td>\n",
       "      <td>1.292</td>\n",
       "      <td>60.68</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.10</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1.136</td>\n",
       "      <td>44.49</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.03</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.642</td>\n",
       "      <td>1.100</td>\n",
       "      <td>28.49</td>\n",
       "      <td>1.478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Simga  Rajim  Rampur  Andhiyarkhore  Ghatora  Bamnidhi  Kurubhata  Kelo  \\\n",
       "0   2.70  0.800     0.0          0.651    1.155     71.48      0.000   0.0   \n",
       "1   2.50  0.900     0.0          0.675    1.183     53.11      0.000   0.0   \n",
       "2   2.17  0.880     0.0          0.664    1.292     60.68      0.000   0.0   \n",
       "3   2.10  0.650     0.0          0.600    1.136     44.49      0.000   0.0   \n",
       "4   2.03  0.652     0.0          0.642    1.100     28.49      1.478   0.0   \n",
       "\n",
       "   Sundargarh  Paramanpur  Basantpur  Hirakud  Mean_areal_rainfall_upper  \n",
       "0       1.417         0.0      45.35      0.0                   0.000000  \n",
       "1       1.313         0.0     125.00      0.0                   0.000000  \n",
       "2       0.935         0.0      81.00      0.0                   0.000000  \n",
       "3       0.873         0.0      98.00      0.0                   0.166895  \n",
       "4       0.810         0.0     117.00      0.0                   0.000000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['hirakud_discharge'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m target_feature \u001b[38;5;241m=\u001b[39m TARGET_FEATURE_NAME\n\u001b[1;32m     42\u001b[0m features_to_use \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]  \u001b[38;5;66;03m# or None to use all except target\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m train_scaled, test_scaled, raw_data_test_target, scaler \u001b[38;5;241m=\u001b[39m get_train_test_data(\n\u001b[1;32m     44\u001b[0m     data, training_percentage, target_feature, features_to_use\n\u001b[1;32m     45\u001b[0m )\n",
      "Cell \u001b[0;32mIn[35], line 23\u001b[0m, in \u001b[0;36mget_train_test_data\u001b[0;34m(data, training_percentage, target_feature, features_to_use)\u001b[0m\n\u001b[1;32m     20\u001b[0m     data_train \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[start_row:\u001b[38;5;241m2000\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Keep raw test target before processing\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m raw_data_test_target \u001b[38;5;241m=\u001b[39m data_test[[target_feature]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Smooth features\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m features_to_use:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['hirakud_discharge'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "def get_train_test_data(data, training_percentage=100, target_feature='target', features_to_use=None):\n",
    "    \n",
    "    # Set default features if not provided\n",
    "    if features_to_use is None:\n",
    "        features_to_use = [col for col in data.columns if col != target_feature]\n",
    "    \n",
    "    # Calculate number of training rows based on percentage\n",
    "    total_train_rows = 2000\n",
    "    train_rows = int((training_percentage / 100) * total_train_rows)\n",
    "    \n",
    "    # Get test data (fixed from row 2000 to end)\n",
    "    data_test = data.iloc[1999:].copy()\n",
    "    \n",
    "    # Get training data (adjusts based on percentage)\n",
    "    if training_percentage == 100:\n",
    "        data_train = data.iloc[:2000].copy()\n",
    "    else:\n",
    "        # Remove initial rows to reduce training data size\n",
    "        start_row = 2000 - train_rows\n",
    "        data_train = data.iloc[start_row:2000].copy()\n",
    "    \n",
    "    # Keep raw test target before processing\n",
    "    raw_data_test_target = data_test[[target_feature]].copy()\n",
    "    \n",
    "    # Smooth features\n",
    "    for col in features_to_use:\n",
    "        window = np.blackman(20)\n",
    "        window /= window.sum()\n",
    "        data_train[col] = np.convolve(data_train[col].values, window, 'same')\n",
    "        data_test[col] = np.convolve(data_test[col].values, window, 'same')\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = scaler.fit_transform(data_train[features_to_use])\n",
    "    test_scaled = scaler.transform(data_test[features_to_use])\n",
    "    \n",
    "    return train_scaled, test_scaled, raw_data_test_target, scaler\n",
    "\n",
    "# Usage:\n",
    "training_percentage = 50\n",
    "target_feature = TARGET_FEATURE_NAME\n",
    "features_to_use = ['feature1', 'feature2', ...]  # or None to use all except target\n",
    "train_scaled, test_scaled, raw_data_test_target, scaler = get_train_test_data(\n",
    "    data, training_percentage, target_feature, features_to_use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(data, training_percentage=100):\n",
    "    # Define the split points based on percentage\n",
    "    split_dates = {\n",
    "        100: '2000-01-01',  # 100% of data -> 4017 days\n",
    "        80: '2002-01-01',   # 80% of data -> 3286 days\n",
    "        50: '2005-01-01',    # 50% of data -> 2190 days\n",
    "        20: '2008-01-01'\n",
    "    }\n",
    "    \n",
    "    if training_percentage not in split_dates:\n",
    "        raise ValueError(\"training_percentage must be one of: 50, 80, or 100\")      # Would like to change to a continous form\n",
    "    \n",
    "    # Split the data\n",
    "    train_start = split_dates[training_percentage]\n",
    "    data_train = data.loc[train_start:'2010-12-31'].copy()\n",
    "    data_test = data.loc['2011-01-01':'2014-12-31'].copy()\n",
    "    \n",
    "    return data_train, data_test\n",
    "\n",
    "# usage:\n",
    "# training_percentage = 50\n",
    "data_train, data_test = get_train_test_split(data, training_percentage)\n",
    "\n",
    "# Keep raw test target before processing\n",
    "raw_data_test_target = data_test[[TARGET_FEATURE_NAME]].copy()\n",
    "\n",
    "# Smooth features\n",
    "for col in FEATURES_TO_USE:\n",
    "    window = np.blackman(20)\n",
    "    data_train[col] = np.convolve(window/window.sum(), data_train[col].values, 'same')\n",
    "    data_test[col] = np.convolve(window/window.sum(), data_test[col].values, 'same')\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(data_train)\n",
    "train_scaled = scaler.transform(data_train)\n",
    "test_scaled = scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the target column in the feature set\n",
    "target_col_index = FEATURES_TO_USE.index(TARGET_FEATURE_NAME)\n",
    "\n",
    "def create_sequences(data, n_steps_in, n_steps_out, target_idx):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps_in - n_steps_out + 1):\n",
    "        X.append(data[i:i+n_steps_in])\n",
    "        y.append(data[i+n_steps_in:i+n_steps_in+n_steps_out, target_idx])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(train_scaled, N_STEPS_IN, N_STEPS_OUT, target_col_index)\n",
    "\n",
    "# Split into train/validation\n",
    "train_X, val_X, train_y, val_y = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_data = TensorDataset(\n",
    "    torch.tensor(train_X, dtype=torch.float32),\n",
    "    torch.tensor(train_y, dtype=torch.float32)\n",
    ")\n",
    "val_data = TensorDataset(\n",
    "    torch.tensor(val_X, dtype=torch.float32),\n",
    "    torch.tensor(val_y, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. MODEL DEFINITION (MLP+MANN) ---\n",
    "class MLP_MANN_Model(nn.Module):\n",
    "    def __init__(self, input_dim, n_steps_in, n_steps_out, memory_size=128, memory_dim=64, hidden_dim=256):\n",
    "        super(MLP_MANN_Model, self).__init__()\n",
    "        flattened_input_dim = n_steps_in * input_dim\n",
    "        self.memory = nn.Parameter(torch.randn(memory_size, memory_dim))\n",
    "        nn.init.xavier_uniform_(self.memory)\n",
    "        self.input_projection = nn.Linear(flattened_input_dim, memory_dim)\n",
    "        self.dense1 = nn.Linear(flattened_input_dim + memory_dim, hidden_dim)\n",
    "        self.dense2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.output_layer = nn.Linear(hidden_dim // 2, n_steps_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        flat_x = x.view(x.size(0), -1)\n",
    "        projected_input = self.input_projection(flat_x)\n",
    "        attention_scores = torch.matmul(projected_input, self.memory.t())\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        memory_output = torch.matmul(attention_weights, self.memory)\n",
    "        augmented_input = torch.cat([flat_x, memory_output], dim=1)\n",
    "        h1 = self.relu(self.dense1(augmented_input))\n",
    "        h2 = self.relu(self.dense2(h1))\n",
    "        output = self.output_layer(h2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, batch_X, batch_y, criterion, optimizer, device):\n",
    "    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(batch_X)\n",
    "    loss = criterion(outputs, batch_y)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for X_batch, y_batch in data_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        total_loss += criterion(outputs, y_batch).item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, criterion, optimizer, device, model_path, save_model=True):\n",
    "    model = model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train': [], 'val': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            train_loss += train_step(model, batch_X, batch_y, criterion, optimizer, device)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Save best model if save_model is True\n",
    "        if save_model and val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "        # Record history\n",
    "        history['train'].append(train_loss)\n",
    "        history['val'].append(val_loss)\n",
    "        \n",
    "        # print(f\"Epoch {epoch+1:3d}/{epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# Training setup\n",
    "input_dim = X.shape[-1]\n",
    "model = MLP_MANN_Model(input_dim, N_STEPS_IN, N_STEPS_OUT)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create model path with region name\n",
    "model_path = MODEL_DIR / f\"best_mlp_mann_model_{REGION}_{training_percentage}pct.pth\"\n",
    "\n",
    "# Train the model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    model_path=str(model_path),\n",
    "    save_model=True  # Set to False to disable model saving\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 6. POST-TRAINING ANALYSIS ---\n",
    "# print(\"\\n--- 6. Plotting Training and Validation Loss ---\")\n",
    "# if history:  # Check if history exists\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.plot(history['train'], label='Train Loss', color='blue')\n",
    "#     plt.plot(history['val'], label='Validation Loss', color='orange')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Loss (MSE)')\n",
    "#     plt.title('MLP+MANN (Hirakud): Training and Validation Loss', fontsize=16)\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. FINAL EVALUATION ---\n",
    "# Create test sequences\n",
    "X_test, y_test_scaled = create_sequences(test_scaled, N_STEPS_IN, N_STEPS_OUT, target_col_index)\n",
    "# print(f\"Testing on {X_test.shape[0]} samples\")\n",
    "\n",
    "# Load best model from the models directory\n",
    "model_path = MODEL_DIR / f\"best_mlp_mann_model_{REGION}_{training_percentage}pct.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device).eval()\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    test_predictions_scaled = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Inverse scaling function\n",
    "def inverse_scale_data(scaled_data, original_scaler, n_features, target_idx):\n",
    "    dummy = np.zeros((scaled_data.size, n_features))\n",
    "    dummy[:, target_idx] = scaled_data.flatten()\n",
    "    return original_scaler.inverse_transform(dummy)[:, target_idx].reshape(scaled_data.shape)\n",
    "\n",
    "# Apply inverse scaling\n",
    "n_features = train_scaled.shape[1]\n",
    "y_test_inv = inverse_scale_data(y_test_scaled, scaler, n_features, target_col_index)\n",
    "test_predictions_inv = inverse_scale_data(test_predictions_scaled, scaler, n_features, target_col_index)\n",
    "\n",
    "# Load raw test data\n",
    "raw_y_test = pd.read_csv(RAW_TEST_FILE_PATH, header=0, dtype=float).values\n",
    "num_samples = raw_y_test.shape[0]\n",
    "# print(f\"Loaded raw test data with {num_samples} samples\")\n",
    "\n",
    "# Align and calculate offset\n",
    "y_test_raw = raw_y_test[:num_samples]\n",
    "y_test_smoothed = y_test_inv[:num_samples]\n",
    "y_predict_smoothed = test_predictions_inv[:num_samples]\n",
    "\n",
    "# Apply offset\n",
    "offset = y_test_raw - y_test_smoothed\n",
    "y_predict_adjusted = y_predict_smoothed + offset\n",
    "# print(\"Offset calculation and adjustment complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(observed, predicted):\n",
    "    \"\"\"Calculate NSE, RSR, PBIAS, EVOL, PE, and TPE metrics.\n",
    "    \n",
    "    Args:\n",
    "        observed: Array of observed values\n",
    "        predicted: Array of predicted values\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (NSE, RSR, PBIAS, EVOL, PE, TPE)\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if len(observed) == 0 or len(predicted) == 0 or len(observed) != len(predicted):\n",
    "        return -np.inf, np.inf, np.inf, np.inf, np.inf, np.inf\n",
    "    \n",
    "    # 1. Calculate NSE (Nash-Sutcliffe Efficiency)\n",
    "    observed_mean = np.mean(observed)\n",
    "    numerator = np.sum((observed - predicted) ** 2)\n",
    "    denominator = np.sum((observed - observed_mean) ** 2)\n",
    "    nse = 1 - (numerator / denominator) if denominator != 0 else -np.inf\n",
    "    \n",
    "    # 2. Calculate RSR (Root Mean Square Error - Observations Standard deviation Ratio)\n",
    "    rsr_numerator = np.sqrt(np.sum((observed - predicted) ** 2))\n",
    "    rsr_denominator = np.sqrt(np.sum((observed - observed_mean) ** 2))\n",
    "    rsr = rsr_numerator / rsr_denominator if rsr_denominator != 0 else np.inf\n",
    "    \n",
    "    # 3. Calculate PBIAS (Percent Bias)\n",
    "    pbias = (np.sum(predicted - observed) / np.sum(observed)) * 100 if np.sum(observed) != 0 else np.inf\n",
    "    \n",
    "    # 4. Calculate EVOL (Volume Error)\n",
    "    sum_observed = np.sum(observed)\n",
    "    sum_predicted = np.sum(predicted)\n",
    "    evol = np.abs(((sum_predicted - sum_observed) / sum_observed) * 100) if sum_observed != 0 else np.inf\n",
    "    \n",
    "    # 5. Calculate PE and TPE (Peak Error and Timing Peak Error)\n",
    "    threshold_obs = np.percentile(observed, 99) if len(observed) > 0 else 0\n",
    "    threshold_sim = np.percentile(predicted, 95) if len(predicted) > 0 else 0\n",
    "    \n",
    "    peaks_obs = find_peaks(observed, height=threshold_obs)[0] if len(observed) > 0 else []\n",
    "    peaks_sim = find_peaks(predicted, height=threshold_sim)[0] if len(predicted) > 0 else []\n",
    "    \n",
    "    tpe_values = []\n",
    "    pe_num = 0\n",
    "    pe_den = 0\n",
    "    \n",
    "    for peak in peaks_obs:\n",
    "        search_range = 5\n",
    "        start = max(0, peak - search_range)\n",
    "        end = min(len(predicted), peak + search_range)\n",
    "        lag = lead = search_range + 1\n",
    "        \n",
    "        if peak in peaks_sim:\n",
    "            tpe_values.append(0)\n",
    "            pe_num += np.abs(predicted[peak] - observed[peak])\n",
    "            pe_den += observed[peak]\n",
    "            continue\n",
    "            \n",
    "        # Find nearest peak in predicted\n",
    "        for i in range(start, end + 1):\n",
    "            if i in peaks_sim:\n",
    "                dist = i - peak\n",
    "                if dist < 0 and abs(dist) < lag:\n",
    "                    lag = abs(dist)\n",
    "                elif dist > 0 and dist < lead:\n",
    "                    lead = dist\n",
    "        \n",
    "        if lead <= search_range or lag <= search_range:\n",
    "            if lead < lag:\n",
    "                tpe_values.append(lead)\n",
    "                pe_idx = peak + lead\n",
    "            else:\n",
    "                tpe_values.append(-lag)\n",
    "                pe_idx = peak - lag\n",
    "                \n",
    "            if 0 <= pe_idx < len(predicted):\n",
    "                pe_num += np.abs(predicted[pe_idx] - observed[peak])\n",
    "                pe_den += observed[peak]\n",
    "    \n",
    "    # Calculate PE (Peak Error)\n",
    "    pe = (pe_num / pe_den) * 100 if pe_den != 0 else np.inf\n",
    "    \n",
    "    # Calculate average TPE (Timing Peak Error)\n",
    "    tpe = np.mean(np.abs(tpe_values)) if tpe_values else 0\n",
    "    \n",
    "    return nse, rsr, pbias, evol, pe, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Region: Hirakud | Training Data: 100%\n",
      "============================================================\n",
      "Test RMSE (Adjusted vs Raw): 729.4561\n",
      "\n",
      "Model Performance Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Step</th>\n",
       "      <th>NSE</th>\n",
       "      <th>RSR</th>\n",
       "      <th>PBIAS (%)</th>\n",
       "      <th>EVOL (%)</th>\n",
       "      <th>PE (%)</th>\n",
       "      <th>TPE (steps)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>0.0940</td>\n",
       "      <td>-3.1876</td>\n",
       "      <td>3.1876</td>\n",
       "      <td>4.9186</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.9836</td>\n",
       "      <td>0.1280</td>\n",
       "      <td>-4.6055</td>\n",
       "      <td>4.6055</td>\n",
       "      <td>6.3278</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>0.1702</td>\n",
       "      <td>-5.9989</td>\n",
       "      <td>5.9989</td>\n",
       "      <td>8.2136</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9471</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>-7.3123</td>\n",
       "      <td>7.3123</td>\n",
       "      <td>11.5455</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.9127</td>\n",
       "      <td>0.2955</td>\n",
       "      <td>-9.4370</td>\n",
       "      <td>9.4370</td>\n",
       "      <td>15.8103</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Step    NSE    RSR  PBIAS (%)  EVOL (%)  PE (%)  TPE (steps)\n",
       "0     1 0.9912 0.0940    -3.1876    3.1876  4.9186       0.0000\n",
       "1     2 0.9836 0.1280    -4.6055    4.6055  6.3278       0.0000\n",
       "2     3 0.9710 0.1702    -5.9989    5.9989  8.2136       0.0000\n",
       "3     4 0.9471 0.2300    -7.3123    7.3123 11.5455       0.0000\n",
       "4     5 0.9127 0.2955    -9.4370    9.4370 15.8103       0.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 8. PERFORMANCE EVALUATION ---\n",
    "# def calculate_metrics(observed, predicted):\n",
    "#     \"\"\"Calculate NSE, RSR, PBIAS, and TPE metrics.\"\"\"\n",
    "#     if np.sum(observed) == 0 or np.sum((observed - np.mean(observed)) ** 2) == 0:\n",
    "#         return -np.inf, np.inf, np.inf, np.inf\n",
    "    \n",
    "#     nse = 1 - (np.sum((observed - predicted) ** 2) / \n",
    "#                np.sum((observed - np.mean(observed)) ** 2))\n",
    "#     rsr = np.sqrt(np.sum((observed - predicted) ** 2)) / \\\n",
    "#           np.sqrt(np.sum((observed - np.mean(observed)) ** 2))\n",
    "#     pbias = (np.sum(predicted - observed) / np.sum(observed)) * 100\n",
    "#     tpe = (np.sum(np.abs(observed - predicted)) / np.sum(observed)) * 100\n",
    "    \n",
    "#     return nse, rsr, pbias, tpe\n",
    "\n",
    "# Calculate overall RMSE\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_raw, y_predict_adjusted))\n",
    "\n",
    "# Print header with region and training percentage\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Region: {REGION} | Training Data: {training_percentage}%\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test RMSE (Adjusted vs Raw): {test_rmse:.4f}\")\n",
    "\n",
    "# Calculate metrics for each prediction step\n",
    "metrics = [calculate_metrics(y_test_raw[:, i], y_predict_adjusted[:, i]) \n",
    "           for i in range(N_STEPS_OUT)]\n",
    "\n",
    "nse_values, rsr_values, pbias_values, evol_values, pe_values, tpe_values = zip(*metrics)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Step': range(1, len(nse_values) + 1),\n",
    "    'NSE': nse_values,\n",
    "    'RSR': rsr_values,\n",
    "    'PBIAS (%)': pbias_values,\n",
    "    'EVOL (%)': evol_values,\n",
    "    'PE (%)': pe_values,\n",
    "    'TPE (steps)': tpe_values\n",
    "})\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Display the metrics\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "display(metrics_df)\n",
    "\n",
    "\n",
    "# # Plot results\n",
    "# plt.figure(figsize=(18, 6))\n",
    "# step_to_plot = 0  # Plotting first prediction step (t+1)\n",
    "# plt.plot(y_test_raw[:, step_to_plot], 'b-', label='Actual (Raw)')\n",
    "# plt.plot(y_predict_adjusted[:, step_to_plot], 'r-', \n",
    "#          label='Predicted (Adjusted)', alpha=0.8)\n",
    "\n",
    "# plt.title(f'Discharge Prediction - Step t+{step_to_plot + 1}')\n",
    "# plt.xlabel(f'Sample Index (n={y_test_raw.shape[0]})')\n",
    "# plt.ylabel('Discharge')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7845267,
     "sourceId": 12437325,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7939697,
     "sourceId": 12572394,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959853,
     "sourceId": 12601992,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959857,
     "sourceId": 12601998,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959868,
     "sourceId": 12602014,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7959881,
     "sourceId": 12602035,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8003950,
     "sourceId": 12665830,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
